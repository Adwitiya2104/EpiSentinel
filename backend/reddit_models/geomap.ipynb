{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8444d9-fca6-4d42-b866-60dc0603ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Fetching Reddit posts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:45<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Reddit posts collected: 2041\n",
      "  query           timestamp platform  \\\n",
      "0   flu 2025-04-14 14:48:16   Reddit   \n",
      "1   flu 2025-04-14 09:20:59   Reddit   \n",
      "2   flu 2025-04-12 20:28:33   Reddit   \n",
      "3   flu 2025-04-09 10:09:46   Reddit   \n",
      "4   flu 2025-04-08 18:30:32   Reddit   \n",
      "\n",
      "                                                text  score  \\\n",
      "0  A new biosensor can detect bird flu in five mi...     13   \n",
      "1  Measles, beef fat, and bird flu: What European...     25   \n",
      "2  North Carolina flu-related deaths at all-time ...     37   \n",
      "3  3-year-old girl in Mexico dies of bird flu in ...     25   \n",
      "4        Bird flu: Mexico reports first human death      44   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.technologyreview.com/2025/04/09/11...  \n",
      "1  https://www.euronews.com/health/2025/04/12/mea...  \n",
      "2  https://www.nbcnews.com/news/us-news/north-car...  \n",
      "3  https://www.euronews.com/health/2025/04/09/3-y...  \n",
      "4  https://www.ctvnews.ca/health/article/mexico-r...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id='',\n",
    "    client_secret='',\n",
    "    user_agent=''\n",
    ")\n",
    "\n",
    "# Full list of search terms\n",
    "search_terms = [\n",
    "    \"flu\", \"fever\", \"rash\", \"covid\", \"epidemic\",\n",
    "    \"cough\", \"RSV\", \"vaccine\", \"pneumonia\", \"chills\",\n",
    "    \"diarrhea\", \"virus\", \"fatigue\", \"contagious\", \"illness\"\n",
    "]\n",
    "\n",
    "reddit_data = []\n",
    "\n",
    "# Loop through queries with progress bar\n",
    "for query in tqdm(search_terms, desc=\"üîç Fetching Reddit posts\"):\n",
    "    for submission in reddit.subreddit('health').search(query, limit=150, sort='new'):\n",
    "        reddit_data.append({\n",
    "            \"query\": query,\n",
    "            \"title\": submission.title,\n",
    "            \"body\": submission.selftext,\n",
    "            \"score\": submission.score,\n",
    "            \"subreddit\": submission.subreddit.display_name,\n",
    "            \"created_utc\": datetime.utcfromtimestamp(submission.created_utc),\n",
    "            \"url\": submission.url,\n",
    "            \"platform\": \"Reddit\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "reddit_df = pd.DataFrame(reddit_data)\n",
    "reddit_df = reddit_df.rename(columns={'created_utc': 'timestamp'})\n",
    "reddit_df[\"text\"] = reddit_df[\"title\"] + \" \" + reddit_df[\"body\"]\n",
    "reddit_df = reddit_df[[\"query\", \"timestamp\", \"platform\", \"text\", \"score\", \"url\"]]\n",
    "\n",
    "print(f\"\\n‚úÖ Reddit posts collected: {len(reddit_df)}\")\n",
    "print(reddit_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686316e2-5c87-4f5a-ac51-6a15fb7576c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üåç Extracting and Geocoding Locations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2041/2041 [00:51<00:00, 39.35it/s]\n",
      "üåç Extracting and Geocoding Locations:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1819/2041 [08:46<01:10,  3.14it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming reddit_df is already defined (example: loaded from a CSV or database)\n",
    "# Example: reddit_df = pd.read_csv(\"reddit_data.csv\")\n",
    "\n",
    "# NLP and Geocoding setup\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "geolocator = Nominatim(user_agent=\"epi_geo\")\n",
    "\n",
    "def extract_location(text):\n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "    return locations[0] if locations else None\n",
    "\n",
    "def geocode_location(location):\n",
    "    try:\n",
    "        loc = geolocator.geocode(location, timeout=10)\n",
    "        if loc:\n",
    "            return (loc.latitude, loc.longitude)\n",
    "    except GeocoderTimedOut:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "tqdm.pandas(desc=\"üåç Extracting and Geocoding Locations\")\n",
    "\n",
    "# Extract locations and geocode them with progress bar\n",
    "reddit_df[\"location\"] = reddit_df[\"text\"].progress_apply(extract_location)\n",
    "reddit_df[\"coords\"] = reddit_df[\"location\"].progress_apply(lambda loc: geocode_location(loc) if pd.notnull(loc) else None)\n",
    "\n",
    "# Filter out rows without valid coordinates\n",
    "reddit_df = reddit_df[reddit_df[\"coords\"].notnull()]\n",
    "\n",
    "# Create heatmap\n",
    "heatmap_data = reddit_df[\"coords\"].tolist()\n",
    "m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "HeatMap(heatmap_data).add_to(m)\n",
    "m.save(\"reddit_heatmap.html\")\n",
    "\n",
    "print(f\"\\nüìç Geo-located posts: {len(reddit_df)}\")\n",
    "print(\"‚úÖ Heatmap saved as: reddit_heatmap.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81568a56-3c30-4109-9f60-1644b46c5256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
